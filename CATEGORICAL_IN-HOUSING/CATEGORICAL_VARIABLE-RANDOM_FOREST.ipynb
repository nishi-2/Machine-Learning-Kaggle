{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b85319b0",
   "metadata": {},
   "source": [
    "### Working with Categorical Variables.\n",
    "##### Categorical variables are those which takes limited number of values. These categorical variables has to be preprocessed before placing the data into the machine learning model.<br>There are three approaches to preprocess (in order to prepare) the categorical data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45c45c48",
   "metadata": {},
   "source": [
    "### First approach -\n",
    "### Drop Categorical Variables -\n",
    "##### The easiest approach to dealing with categorical variables is to simply remove them from the dataset. This approach will only work well if the columns did not contain useful information.\n",
    "### Ordinal Encoding -\n",
    "##### This approach is assigning unique values to the categories, thereby making an ordering of the categories. For example,<br>an ordering of the categories: \"Never\" (0) < \"Rarely\" (1) < \"Most days\" (2) < \"Every day\" (3).\n",
    "### One-Hot Encoding -\n",
    "##### One-hot encoding creates new columns indicating the presence (or absence) of each possible value in the original data.\n",
    "In contrast to ordinal encoding, one-hot encoding does not assume an ordering of the categories. Thus, you can expect this approach to work particularly well if there is no clear ordering in the categorical data. We refer to categorical variables without an intrinsic ranking as nominal variables.\n",
    "<br><br>\n",
    "One-hot encoding generally does not perform well if the categorical variable takes on a large number of values (i.e., you generally won't use it for variables taking more than 15 different values)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9bc0711e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "959646c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Suburb</th>\n",
       "      <th>Address</th>\n",
       "      <th>Rooms</th>\n",
       "      <th>Type</th>\n",
       "      <th>Method</th>\n",
       "      <th>SellerG</th>\n",
       "      <th>Date</th>\n",
       "      <th>Distance</th>\n",
       "      <th>Postcode</th>\n",
       "      <th>Bedroom2</th>\n",
       "      <th>Bathroom</th>\n",
       "      <th>Car</th>\n",
       "      <th>Landsize</th>\n",
       "      <th>BuildingArea</th>\n",
       "      <th>YearBuilt</th>\n",
       "      <th>CouncilArea</th>\n",
       "      <th>Lattitude</th>\n",
       "      <th>Longtitude</th>\n",
       "      <th>Regionname</th>\n",
       "      <th>Propertycount</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Abbotsford</td>\n",
       "      <td>85 Turner St</td>\n",
       "      <td>2</td>\n",
       "      <td>h</td>\n",
       "      <td>S</td>\n",
       "      <td>Biggin</td>\n",
       "      <td>3/12/2016</td>\n",
       "      <td>2.5</td>\n",
       "      <td>3067.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>202.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Yarra</td>\n",
       "      <td>-37.7996</td>\n",
       "      <td>144.9984</td>\n",
       "      <td>Northern Metropolitan</td>\n",
       "      <td>4019.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Abbotsford</td>\n",
       "      <td>25 Bloomburg St</td>\n",
       "      <td>2</td>\n",
       "      <td>h</td>\n",
       "      <td>S</td>\n",
       "      <td>Biggin</td>\n",
       "      <td>4/02/2016</td>\n",
       "      <td>2.5</td>\n",
       "      <td>3067.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>156.0</td>\n",
       "      <td>79.0</td>\n",
       "      <td>1900.0</td>\n",
       "      <td>Yarra</td>\n",
       "      <td>-37.8079</td>\n",
       "      <td>144.9934</td>\n",
       "      <td>Northern Metropolitan</td>\n",
       "      <td>4019.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Suburb          Address  Rooms Type Method SellerG       Date  \\\n",
       "0  Abbotsford     85 Turner St      2    h      S  Biggin  3/12/2016   \n",
       "1  Abbotsford  25 Bloomburg St      2    h      S  Biggin  4/02/2016   \n",
       "\n",
       "   Distance  Postcode  Bedroom2  Bathroom  Car  Landsize  BuildingArea  \\\n",
       "0       2.5    3067.0       2.0       1.0  1.0     202.0           NaN   \n",
       "1       2.5    3067.0       2.0       1.0  0.0     156.0          79.0   \n",
       "\n",
       "   YearBuilt CouncilArea  Lattitude  Longtitude             Regionname  \\\n",
       "0        NaN       Yarra   -37.7996    144.9984  Northern Metropolitan   \n",
       "1     1900.0       Yarra   -37.8079    144.9934  Northern Metropolitan   \n",
       "\n",
       "   Propertycount  \n",
       "0         4019.0  \n",
       "1         4019.0  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_path = 'C:/Users/hp/Desktop/MACHINE LEARNING/CATEGORICAL_IN-HOUSING/train.csv'\n",
    "house_data = pd.read_csv(file_path)\n",
    "\n",
    "y = house_data.Price\n",
    "X = house_data.drop(['Price'],axis=1)\n",
    "X.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "99aadc5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X, test_X, train_y, test_y = train_test_split(X,y,train_size=0.8,test_size=0.2,random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fc73bb5",
   "metadata": {},
   "source": [
    "Now, the columns that are having null values are found and dropped from the table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "321d285d",
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_columns = [col for col in train_X.columns if train_X[col].isnull().any()]\n",
    "train_X.drop(missing_columns, axis=1, inplace=True)\n",
    "test_X.drop(missing_columns, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "387950d1",
   "metadata": {},
   "source": [
    "So, the columns having null values are removed. Now categorical column has to be selected with low cardinality.<br>\n",
    "##### What is Cardinality?<br> Cardinality means the number of unique values in a column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "aa90bffb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Type', 'Method', 'Regionname']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "low_card_col = [col for col in train_X.columns if train_X[col].nunique()<10 and train_X[col].dtype=='object']\n",
    "low_card_col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2e44a273",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Rooms',\n",
       " 'Distance',\n",
       " 'Postcode',\n",
       " 'Bedroom2',\n",
       " 'Bathroom',\n",
       " 'Landsize',\n",
       " 'Lattitude',\n",
       " 'Longtitude',\n",
       " 'Propertycount']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "numerical_cols = [col for col in train_X.columns if train_X[col].dtype in ['int64','float64']]\n",
    "numerical_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a3951063",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Type',\n",
       " 'Method',\n",
       " 'Regionname',\n",
       " 'Rooms',\n",
       " 'Distance',\n",
       " 'Postcode',\n",
       " 'Bedroom2',\n",
       " 'Bathroom',\n",
       " 'Landsize',\n",
       " 'Lattitude',\n",
       " 'Longtitude',\n",
       " 'Propertycount']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_cols = low_card_col + numerical_cols\n",
    "my_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "adfa2b28",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X_new = train_X[my_cols].copy()\n",
    "test_X_new = test_X[my_cols].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d56cf0e0",
   "metadata": {},
   "source": [
    "Next, we obtain a list of all of the categorical variables in the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "8252b14a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Type', 'Method', 'Regionname']\n"
     ]
    }
   ],
   "source": [
    "s = (train_X_new.dtypes=='object')\n",
    "object_cols = list(s[s].index)\n",
    "print(object_cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae342113",
   "metadata": {},
   "source": [
    "### So, now we have got categorical variables. Next, we define a function to compare the three different approaches to dealing with categorical variables. This function reports the mean absolute error (MAE) from a random forest model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d83a60a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def score_dataset(train_X, test_X, train_y, test_y):\n",
    "    model = RandomForestRegressor(n_estimators=100, random_state=0)\n",
    "    model.fit(train_X,train_y)\n",
    "    prediction = model.predict(test_X)\n",
    "    return mean_absolute_error(test_y,prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08c03b56",
   "metadata": {},
   "source": [
    "### First approach of DROP CATEGORICAL VARIABLE -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "2e9a8406",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X_drop = train_X_new.select_dtypes(exclude=['object'])\n",
    "test_X_drop = test_X_new.select_dtypes(exclude=['object'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "b3d2ab51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE from approach 1 after dropping the categorical variables:   \n",
      "175703.48185157913\n"
     ]
    }
   ],
   "source": [
    "print('MAE from approach 1 after dropping the categorical variables:   ')\n",
    "print(score_dataset(train_X_drop, test_X_drop, train_y, test_y))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "635a5ac4",
   "metadata": {},
   "source": [
    "### Second approach of ORDINAL ENCODING -\n",
    "##### Scikit-learn has a OrdinalEncoder class that can be used to get ordinal encodings. We loop over the categorical variables and apply the ordinal encoder separately to each column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "fdbee38d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "\n",
    "train_X_label = train_X_new.copy()\n",
    "test_X_label = test_X_new.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "d0ac779a",
   "metadata": {},
   "outputs": [],
   "source": [
    "ordinalencoder = OrdinalEncoder()\n",
    "train_X_label[object_cols] = ordinalencoder.fit_transform(train_X_new[object_cols])\n",
    "test_X_label[object_cols] = ordinalencoder.transform(test_X_new[object_cols])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "aec0badf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE from approach 2 after ordinal encoding:   \n",
      "165936.40548390493\n"
     ]
    }
   ],
   "source": [
    "print('MAE from approach 2 after ordinal encoding:   ')\n",
    "print(score_dataset(train_X_label, test_X_label, train_y, test_y))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b6c6a4e",
   "metadata": {},
   "source": [
    "### Third approach of ONE-HOT ENCODING -\n",
    "#####  - OneHotEncoder class from scikit-learn to get one-hot encodings <br> - set handle_unknown='ignore' to avoid errors when the validation data contains classes that aren't represented in the training data <br> - setting sparse=False ensures that the encoded columns are returned as a numpy array (instead of a sparse matrix) <br> - To use the encoder, we supply only the categorical columns that we want to be one-hot encoded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "82202fc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "oh_encoder = OneHotEncoder(handle_unknown='ignore', sparse=False)\n",
    "train_ohencode = pd.DataFrame(oh_encoder.fit_transform(train_X_new[object_cols]))\n",
    "test_ohencode = pd.DataFrame(oh_encoder.fit_transform(test_X_new[object_cols]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e944985c",
   "metadata": {},
   "source": [
    "One-hot encoding removed index; put it back"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "4ce66d38",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ohencode.index = train_X_new.index\n",
    "test_ohencode.index = test_X_new.index"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "802bcb33",
   "metadata": {},
   "source": [
    "Remove categorical columns (will replace with one-hot encoding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "3b9cfe8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_X_train = train_X_new.drop(object_cols, axis=1)\n",
    "num_X_test = test_X_new.drop(object_cols, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15736b72",
   "metadata": {},
   "source": [
    "Add one-hot encoded columns to numerical features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "6ddae372",
   "metadata": {},
   "outputs": [],
   "source": [
    "OH_X_train = pd.concat([num_X_train, train_ohencode], axis=1)\n",
    "OH_X_test = pd.concat([num_X_test, test_ohencode], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "c01307f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE from approach 3 after one hot encoding:   \n",
      "166089.4893009678\n"
     ]
    }
   ],
   "source": [
    "print('MAE from approach 3 after one hot encoding:   ')\n",
    "print(score_dataset(OH_X_train, OH_X_test, train_y, test_y))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e31a15c",
   "metadata": {},
   "source": [
    "In this case, dropping the categorical columns (Approach 1) performed worst, since it had the highest MAE score. As for the other two approaches, since the returned MAE scores are so close in value, there doesn't appear to be any meaningful benefit to one over the other.\n",
    "<br>\n",
    "In general, one-hot encoding (Approach 3) will typically perform best, and dropping the categorical columns (Approach 1) typically performs worst, but it varies on a case-by-case basis."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
